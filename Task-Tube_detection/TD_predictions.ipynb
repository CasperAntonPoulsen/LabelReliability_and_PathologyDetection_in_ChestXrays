{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecc6ed7",
   "metadata": {},
   "source": [
    "# Model predictions for the tube detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e9b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eef5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data\n",
    "\n",
    "# Model predictions\n",
    "TD_preds = pd.read_csv('Predictions/TD_preds.csv', index_col=0)\n",
    "TD_preds_CXR14 = pd.read_csv('Predictions/TD_preds_CXR14.csv', index_col=0)\n",
    "\n",
    "# True labels\n",
    "true_labels = pd.read_csv('../Data/Data_splits/tube_detection-test.csv', index_col=0)\n",
    "true_labels_CXR14 = pd.read_csv('../Data/Data_splits/tube_detection-CXR14_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c49d3",
   "metadata": {},
   "source": [
    "## Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a2d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading the predictions, which are strings, as numpy arrays\n",
    "def str2array(s):\n",
    "    # Remove space after [\n",
    "    s=re.sub('\\[ +', '[', s.strip())\n",
    "    # Replace commas and spaces\n",
    "    s=re.sub('[,\\s]+', ', ', s)\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "# Function for printing the average accuracy and auc (with std) for tube detection task for chest drains\n",
    "def get_preds_binary(df, y_true_anns, y_true_padchest):\n",
    "    all_preds = []\n",
    "\n",
    "    for row_number in range(len(df)):\n",
    "        #print(row_number)\n",
    "        preds = [[str2array(i[\"Preds_model1\"]) for idx, i in df.iterrows()][row_number][:,1],\n",
    "                [str2array(i[\"Preds_model2\"]) for idx, i in df.iterrows()][row_number][:,1],\n",
    "                [str2array(i[\"Preds_model3\"]) for idx, i in df.iterrows()][row_number][:,1]]\n",
    "        all_preds.append(preds)\n",
    "        \n",
    "        \n",
    "    #print(len(all_preds[0]))\n",
    "    #print(all_preds[0][0][:10])\n",
    "    \n",
    "    # Constructing a df with the preds and 'true' labels\n",
    "    for idx, row in df.reset_index(drop=True).iterrows():\n",
    "        #print('DF ROW: ', idx)\n",
    "        print('Model: ', row['Model_name'])\n",
    "        #print('Val data: ', row['Val_data'])\n",
    "        preds_model1 = all_preds[idx][0]\n",
    "        preds_model2 = all_preds[idx][1]\n",
    "        preds_model3 = all_preds[idx][2]\n",
    "        \n",
    "        # Getting rid of the damn -1 anns\n",
    "        preds_df = pd.DataFrame(list(zip(y_true_anns,\n",
    "                                         y_true_padchest,\n",
    "                                         list(preds_model1),\n",
    "                                         list(preds_model2),\n",
    "                                         list(preds_model3))),\n",
    "                                columns =['Anns', 'PadChest', 'preds_model1', 'preds_model2', 'preds_model3'])\n",
    "        \n",
    "        preds_df = preds_df[preds_df['Anns'] != -1]\n",
    "\n",
    "        # Computing the performance scores\n",
    "        auc_with_anns = [roc_auc_score(preds_df['Anns'], preds_df['preds_model1']), roc_auc_score(preds_df['Anns'], preds_df['preds_model2']), roc_auc_score(preds_df['Anns'], preds_df['preds_model3'])]\n",
    "        auc_with_padchest = [roc_auc_score(preds_df['PadChest'], preds_df['preds_model1']), roc_auc_score(preds_df['PadChest'], preds_df['preds_model2']), roc_auc_score(preds_df['PadChest'], preds_df['preds_model3'])]\n",
    "        \n",
    "        print(\"Annotations Average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "        print(\"PadChest Average auc:\", round(sum(auc_with_padchest)/len(auc_with_padchest)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_padchest)*100,5))\n",
    "        print()\n",
    "        \n",
    "        # Printing a confusion matrix\n",
    "        #print(confusion_matrix(preds_df['Anns'], preds_df['preds_model1']))  # Cannot, due to non-integer probs\n",
    "\n",
    "\n",
    "# Function to arrange preds nicely in a df\n",
    "def get_perf_only_chd(orig_pred_df, true_labels_df, print_auc=True):\n",
    "    tube_types = ['Chest_drain_tube', 'NSG_tube', 'Endotracheal_tube', 'Tracheostomy_tube']\n",
    "    all_preds = []\n",
    "    \n",
    "    for row_number in range(len(orig_pred_df)):\n",
    "        for idx, tube in enumerate(tube_types):\n",
    "            preds = [[str2array(i[\"Preds_model1\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx],\n",
    "                     [str2array(i[\"Preds_model2\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx],\n",
    "                     [str2array(i[\"Preds_model3\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx]]\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "    # Constructing a df with the preds and 'true' labels\n",
    "    preds_df = pd.DataFrame(list(zip(list(true_labels_df['Chest_drain_Ann']),\n",
    "                                     list(all_preds[0][0]),\n",
    "                                     list(all_preds[0][1]),\n",
    "                                     list(all_preds[0][2]))),\n",
    "                            columns = ['Chest_drain_Ann',\n",
    "                                       'preds_CheD_model1', 'preds_CheD_model2', 'preds_CheD_model3'])\n",
    "\n",
    "    if print_auc:\n",
    "        # Computing the auc for each tube separately\n",
    "        print('CHEST DRAIN TUBE')\n",
    "        auc_with_anns = [roc_auc_score(preds_df['Chest_drain_Ann'], preds_df['preds_CheD_model1']), roc_auc_score(preds_df['Chest_drain_Ann'], preds_df['preds_CheD_model2']), roc_auc_score(preds_df['Chest_drain_Ann'], preds_df['preds_CheD_model3'])]\n",
    "        print(\"Annotations average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "    \n",
    "    return preds_df\n",
    "\n",
    "\n",
    "# Function to arrange preds nicely in a df\n",
    "def get_preds_multiclass(orig_pred_df, true_labels_df, print_auc=True):\n",
    "    tube_types = ['Chest_drain_tube', 'NSG_tube', 'Endotracheal_tube', 'Tracheostomy_tube']\n",
    "    all_preds = []\n",
    "    \n",
    "    for row_number in range(len(orig_pred_df)):\n",
    "        for idx, tube in enumerate(tube_types):\n",
    "            preds = [[str2array(i[\"Preds_model1\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx],\n",
    "                     [str2array(i[\"Preds_model2\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx],\n",
    "                     [str2array(i[\"Preds_model3\"]) for idx, i in orig_pred_df.iterrows()][row_number][:,idx]]\n",
    "            all_preds.append(preds)\n",
    "            \n",
    "    # Constructing a df with the preds and 'true' labels\n",
    "    preds_df = pd.DataFrame(list(zip(list(true_labels_df['Chest_drain_Ann']),\n",
    "                                     list(true_labels_df['NSG_tube_Ann']),\n",
    "                                     list(true_labels_df['Endotracheal_tube_Ann']),\n",
    "                                     list(true_labels_df['Tracheostomy_tube_Ann']),\n",
    "                                     list(true_labels_df['Chest_drain_tube']),\n",
    "                                     list(true_labels_df['NSG_tube']),\n",
    "                                     list(true_labels_df['Endotracheal_tube']),\n",
    "                                     list(true_labels_df['Tracheostomy_tube']),\n",
    "                                     list(all_preds[0][0]),\n",
    "                                     list(all_preds[0][1]),\n",
    "                                     list(all_preds[0][2]),\n",
    "                                     list(all_preds[1][0]),\n",
    "                                     list(all_preds[1][1]),\n",
    "                                     list(all_preds[1][2]),\n",
    "                                     list(all_preds[2][0]),\n",
    "                                     list(all_preds[2][1]),\n",
    "                                     list(all_preds[2][2]),\n",
    "                                     list(all_preds[3][0]),\n",
    "                                     list(all_preds[3][1]),\n",
    "                                     list(all_preds[3][2]))),\n",
    "                            columns = ['Chest_drain_Ann', 'NSG_tube_Ann', 'Endotracheal_tube_Ann', 'Tracheostomy_tube_Ann',\n",
    "                                       'Chest_drain_tube_PadChest', 'NSG_tube_PadChest', 'Endotracheal_tube_PadChest', 'Tracheostomy_tube_PadChest',\n",
    "                                       'preds_CheD_model1', 'preds_CheD_model2', 'preds_CheD_model3',\n",
    "                                       'preds_NSG_model1', 'preds_NSG_model2', 'preds_NSG_model3',\n",
    "                                       'preds_Endo_model1', 'preds_Endo_model2', 'preds_Endo_model3',\n",
    "                                       'preds_Trach_model1', 'preds_Trach_model2', 'preds_Trach_model3',])\n",
    "\n",
    "\n",
    "    ## From here, one can return the preds_df if you want to see the predictions nicely\n",
    "    \n",
    "    if print_auc:\n",
    "        # Computing the auc for each tube separately\n",
    "        print('CHEST DRAIN TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Chest_drain_Ann'] != -1]\n",
    "        auc_with_anns = [roc_auc_score(preds_df_tube['Chest_drain_Ann'], preds_df_tube['preds_CheD_model1']), roc_auc_score(preds_df_tube['Chest_drain_Ann'], preds_df_tube['preds_CheD_model2']), roc_auc_score(preds_df_tube['Chest_drain_Ann'], preds_df_tube['preds_CheD_model3'])]\n",
    "        auc_with_padchest = [roc_auc_score(preds_df_tube['Chest_drain_tube_PadChest'], preds_df_tube['preds_CheD_model1']), roc_auc_score(preds_df_tube['Chest_drain_tube_PadChest'], preds_df_tube['preds_CheD_model2']), roc_auc_score(preds_df_tube['Chest_drain_tube_PadChest'], preds_df_tube['preds_CheD_model3'])]\n",
    "        print(\"Annotations Average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "        print(\"PadChest Average auc:\", round(sum(auc_with_padchest)/len(auc_with_padchest)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_padchest)*100,5))\n",
    "        #print(auc_with_anns)\n",
    "        #print(auc_with_padchest)\n",
    "        print()\n",
    "\n",
    "        print('NSG TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['NSG_tube_Ann'] != -1]\n",
    "        auc_with_anns = [roc_auc_score(preds_df_tube['NSG_tube_Ann'], preds_df_tube['preds_NSG_model1']), roc_auc_score(preds_df_tube['NSG_tube_Ann'], preds_df_tube['preds_NSG_model2']), roc_auc_score(preds_df_tube['NSG_tube_Ann'], preds_df_tube['preds_NSG_model3'])]\n",
    "        auc_with_padchest = [roc_auc_score(preds_df_tube['NSG_tube_PadChest'], preds_df_tube['preds_NSG_model1']), roc_auc_score(preds_df_tube['NSG_tube_PadChest'], preds_df_tube['preds_NSG_model2']), roc_auc_score(preds_df_tube['NSG_tube_PadChest'], preds_df_tube['preds_NSG_model3'])]\n",
    "        print(\"Annotations Average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "        print(\"PadChest Average auc:\", round(sum(auc_with_padchest)/len(auc_with_padchest)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_padchest)*100,5))\n",
    "        #print(auc_with_anns)\n",
    "        #print(auc_with_padchest)\n",
    "        print()\n",
    "\n",
    "        print('ENDOTRACHEAL TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Endotracheal_tube_Ann'] != -1]\n",
    "        auc_with_anns = [roc_auc_score(preds_df_tube['Endotracheal_tube_Ann'], preds_df_tube['preds_Endo_model1']), roc_auc_score(preds_df_tube['Endotracheal_tube_Ann'], preds_df_tube['preds_Endo_model2']), roc_auc_score(preds_df_tube['Endotracheal_tube_Ann'], preds_df_tube['preds_Endo_model3'])]\n",
    "        auc_with_padchest = [roc_auc_score(preds_df_tube['Endotracheal_tube_PadChest'], preds_df_tube['preds_Endo_model1']), roc_auc_score(preds_df_tube['Endotracheal_tube_PadChest'], preds_df_tube['preds_Endo_model2']), roc_auc_score(preds_df_tube['Endotracheal_tube_PadChest'], preds_df_tube['preds_Endo_model3'])]\n",
    "        print(\"Annotations Average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "        print(\"PadChest Average auc:\", round(sum(auc_with_padchest)/len(auc_with_padchest)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_padchest)*100,5))\n",
    "        #print(auc_with_anns)\n",
    "        #print(auc_with_padchest)\n",
    "        print()\n",
    "\n",
    "        print('TRACHEOSTOMY TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Tracheostomy_tube_Ann'] != -1]\n",
    "        auc_with_anns = [roc_auc_score(preds_df_tube['Tracheostomy_tube_Ann'], preds_df_tube['preds_Trach_model1']), roc_auc_score(preds_df_tube['Tracheostomy_tube_Ann'], preds_df_tube['preds_Trach_model2']), roc_auc_score(preds_df_tube['Tracheostomy_tube_Ann'], preds_df_tube['preds_Trach_model3'])]\n",
    "        auc_with_padchest = [roc_auc_score(preds_df_tube['Tracheostomy_tube_PadChest'], preds_df_tube['preds_Trach_model1']), roc_auc_score(preds_df_tube['Tracheostomy_tube_PadChest'], preds_df_tube['preds_Trach_model2']), roc_auc_score(preds_df_tube['Tracheostomy_tube_PadChest'], preds_df_tube['preds_Trach_model3'])]\n",
    "        print(\"Annotations Average auc:\", round(sum(auc_with_anns)/len(auc_with_anns)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_anns)*100,5))\n",
    "        print(\"PadChest Average auc:\", round(sum(auc_with_padchest)/len(auc_with_padchest)*100, 5), \"with standard deviation:\", round(statistics.stdev(auc_with_padchest)*100,5))\n",
    "        #print(auc_with_anns)\n",
    "        #print(auc_with_padchest)\n",
    "        print()\n",
    "    \n",
    "    return preds_df\n",
    "\n",
    "\n",
    "# Function for printing the average accuracy and auc (with std) for tube detection task for chest drains\n",
    "def get_preds_multiclass_one_model(df, true_labels_df, print_auc=True):\n",
    "    tube_types = ['Chest_drain_tube', 'NSG_tube', 'Endotracheal_tube', 'Tracheostomy_tube']\n",
    "    all_preds = []\n",
    "    \n",
    "    for row_number in range(len(df)):\n",
    "        for t_idx, tube in enumerate(tube_types):\n",
    "            preds = [[str2array(i[\"Preds_model1\"]) for idx, i in df.iterrows()][row_number][:,t_idx]]\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    \n",
    "    # Constructing a df with the preds and 'true' labels\n",
    "    preds_df = pd.DataFrame(list(zip(list(true_labels_df['Chest_drain_Ann']),\n",
    "                                     list(true_labels_df['NSG_tube_Ann']),\n",
    "                                     list(true_labels_df['Endotracheal_tube_Ann']),\n",
    "                                     list(true_labels_df['Tracheostomy_tube_Ann']),\n",
    "                                     list(true_labels_df['Chest_drain_tube']),\n",
    "                                     list(true_labels_df['NSG_tube']),\n",
    "                                     list(true_labels_df['Endotracheal_tube']),\n",
    "                                     list(true_labels_df['Tracheostomy_tube']),\n",
    "                                     list(all_preds[0][0]),\n",
    "                                     list(all_preds[1][0]),\n",
    "                                     list(all_preds[2][0]),\n",
    "                                     list(all_preds[3][0]))),\n",
    "                            columns = ['Chest_drain_Ann', 'NSG_tube_Ann', 'Endotracheal_tube_Ann', 'Tracheostomy_tube_Ann',\n",
    "                                       'Chest_drain_tube_PadChest', 'NSG_tube_PadChest', 'Endotracheal_tube_PadChest', 'Tracheostomy_tube_PadChest',\n",
    "                                       'preds_CheD_model1', 'preds_NSG_model1',\n",
    "                                       'preds_Endo_model1', 'preds_Trach_model1'])\n",
    "\n",
    "    \n",
    "    ## From here, one can return the preds_df if you want to see the predictions nicely\n",
    "    \n",
    "    if print_auc:\n",
    "        # Computing the auc for each tube separately\n",
    "        print('CHEST DRAIN TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Chest_drain_Ann'] != -1]\n",
    "        print(\"Annotations auc:\", round(roc_auc_score(preds_df_tube['Chest_drain_Ann'], preds_df_tube['preds_CheD_model1'])*100, 5))\n",
    "        print(\"PadChest auc:\", round(roc_auc_score(preds_df_tube['Chest_drain_tube_PadChest'], preds_df_tube['preds_CheD_model1'])*100, 5))\n",
    "        print()\n",
    "\n",
    "        print('NSG TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['NSG_tube_Ann'] != -1]\n",
    "        print(\"Annotations auc:\", round(roc_auc_score(preds_df_tube['NSG_tube_Ann'], preds_df_tube['preds_NSG_model1'])*100, 5))\n",
    "        print(\"PadChest auc:\", round(roc_auc_score(preds_df_tube['NSG_tube_PadChest'], preds_df_tube['preds_NSG_model1'])*100, 5))\n",
    "        print()\n",
    "\n",
    "        print('ENDOTRACHEAL TUBE')\n",
    "        print('NSG TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Endotracheal_tube_Ann'] != -1]\n",
    "        print(\"Annotations auc:\", round(roc_auc_score(preds_df_tube['Endotracheal_tube_Ann'], preds_df_tube['preds_Endo_model1'])*100, 5))\n",
    "        print(\"PadChest auc:\", round(roc_auc_score(preds_df_tube['Endotracheal_tube_PadChest'], preds_df_tube['preds_Endo_model1'])*100, 5))\n",
    "        print()\n",
    "\n",
    "        print('TRACHEOSTOMY TUBE')\n",
    "        preds_df_tube = preds_df[preds_df['Tracheostomy_tube_Ann'] != -1]\n",
    "        print(\"Annotations auc:\", round(roc_auc_score(preds_df_tube['Tracheostomy_tube_Ann'], preds_df_tube['preds_Trach_model1'])*100, 5))\n",
    "        print(\"PadChest auc:\", round(roc_auc_score(preds_df_tube['Tracheostomy_tube_PadChest'], preds_df_tube['preds_Trach_model1'])*100, 5))\n",
    "        print()\n",
    "    \n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d867cc8",
   "metadata": {},
   "source": [
    "### Multiclass: DenseNet121 fine-tuned on PadChest, detecting 4 tubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a51ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHEST DRAIN TUBE\n",
      "Annotations Average auc: 83.73617 with standard deviation: 4.5467\n",
      "PadChest Average auc: 77.79 with standard deviation: 3.46581\n",
      "\n",
      "NSG TUBE\n",
      "Annotations Average auc: 68.36736 with standard deviation: 1.11814\n",
      "PadChest Average auc: 74.83157 with standard deviation: 1.26568\n",
      "\n",
      "ENDOTRACHEAL TUBE\n",
      "Annotations Average auc: 74.86049 with standard deviation: 0.41856\n",
      "PadChest Average auc: 75.73033 with standard deviation: 0.06881\n",
      "\n",
      "TRACHEOSTOMY TUBE\n",
      "Annotations Average auc: 87.5752 with standard deviation: 2.04351\n",
      "PadChest Average auc: 86.43756 with standard deviation: 2.5167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_df = get_preds_multiclass(TD_preds, true_labels)\n",
    "#preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c5414",
   "metadata": {},
   "source": [
    "### Single class: DenseNet121 fine-tuned on PadChest, evaluated on CXR14, detecting only chest drains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63941df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHEST DRAIN TUBE\n",
      "Annotations average auc: 48.42036 with standard deviation: 1.77075\n"
     ]
    }
   ],
   "source": [
    "preds_df = get_perf_only_chd(TD_preds_CXR14, true_labels_CXR14)\n",
    "#preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6dc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7257aa5",
   "metadata": {},
   "source": [
    "## Implementation of Class-Wise Calibration Error (CWCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e6c2e",
   "metadata": {},
   "source": [
    "Binary Expected Calibration Error:\n",
    "$$ \\text{binary-ECE}  = \\sum_{i=1}^M \\frac{|B_{i}|}{N} |\n",
    "        \\bar{y}(B_{i}) - \\bar{p}(B_{i})| $$\n",
    "\n",
    "Class-wise Expected Calibration Error:\n",
    "$$ \\text{class-$j$-ECE}  = \\sum_{i=1}^M \\frac{|B_{i,j}|}{N}\n",
    "        |\\bar{y}_j(B_{i,j}) - \\bar{p}_j(B_{i,j})|,\n",
    "        \\text{classwise-ECE}  = \\frac{1}{K}\\sum_{j=1}^K \\text{class-$j$-ECE} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2253763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_ECE(y_true, probs, power=1, bins=10):\n",
    "    r\"\"\"\n",
    "    Binary Expected Calibration Error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : indicator vector (n_samples, )\n",
    "        True labels.\n",
    "    probs : matrix (n_samples, )\n",
    "        Predicted probabilities for positive class.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "    \"\"\"\n",
    "\n",
    "    create_bins = np.linspace(start=0, stop=1, num=bins + 1)   # Returns 'num' evenly spaced samples, calculated over the interval [start, stop]\n",
    "    #print('bins created: ', create_bins)\n",
    "    idx_bins = np.digitize(x=probs, bins=create_bins)   # Return the indices of the bins to which each value in input array belongs\n",
    "    idx_bins -= 1   # Need to subtract 1 from the bin indices to start at 0\n",
    "    \n",
    "    \n",
    "    # Function for computing the ECE for one bin\n",
    "    def bin_func(y, p, idx_bins):\n",
    "        probs_bin_mean = np.mean(p[idx_bins])   # Mean of probs in bin i\n",
    "        true_bin_mean = np.mean(y[idx_bins])   # Mean of true values in bin i\n",
    "        diff = np.abs(probs_bin_mean - true_bin_mean)   # Absolute difference between the two bin means\n",
    "        diff_power = diff ** power   # Raising the diff according to the L_p calibration error specified, typically power = 1\n",
    "        ece = diff_power * np.sum(idx_bins) / len(p)   # Multiplying by the fraction of probs in that bin\n",
    "        return ece\n",
    "        \n",
    "    # Computing the binary ECE for each bin and summing them\n",
    "    ece = 0\n",
    "    \n",
    "    for i in np.unique(idx_bins):   # Looping through the unique bins (len(bins))\n",
    "        ece += bin_func(y_true, probs, idx_bins == i)   # Summing the error for each bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "\n",
    "def classwise_ECE(y_true, probs, classes_list, power=1, bins=10, print_ece=False):\n",
    "    r\"\"\"Classwise Expected Calibration Error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : label indicator matrix (n_samples, n_classes)\n",
    "        True labels.\n",
    "    probs : matrix (n_samples, n_classes)\n",
    "        Predicted probabilities.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "    \"\"\"\n",
    "\n",
    "    n_classes = len(classes_list)\n",
    "    \n",
    "    # Computing the binary ECE for each class\n",
    "    class_eces = []\n",
    "    for c in range(n_classes):   # Looping through the classes\n",
    "        binary_ece = binary_ECE(y_true[:, c], probs[:, c], power=power, bins=bins)\n",
    "        if print_ece:\n",
    "            print('ECE for {}: {}'.format(classes_list[c], round(binary_ece, 3)))\n",
    "        class_eces.append(binary_ece)\n",
    "    \n",
    "    #if print_ece:\n",
    "        #print()\n",
    "        #print('Average Class-Wise ECE: ', round(np.mean(class_eces), 3))\n",
    "    \n",
    "    return class_eces\n",
    "    # Right now, not printing the average class-wise calibration error\n",
    "\n",
    "    \n",
    "def classwise_ECE_three_models(df_orig, df_y_true, classes_list, power=1, bins=10):\n",
    "        \n",
    "    # Creating the preds df\n",
    "    preds_df = get_preds_multiclass(df_orig, df_y_true, print_auc=False)\n",
    "    all_model_eces_ann = []\n",
    "    all_model_eces_pad = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        probs_model_df = preds_df[['preds_CheD_model'+str(i+1), 'preds_NSG_model'+str(i+1), 'preds_Endo_model'+str(i+1), 'preds_Trach_model'+str(i+1)]]\n",
    "        y_true_ann_df = preds_df[['Chest_drain_Ann', 'NSG_tube_Ann', 'Endotracheal_tube_Ann', 'Tracheostomy_tube_Ann']]\n",
    "        y_true_pad_df = preds_df[['Chest_drain_tube_PadChest', 'NSG_tube_PadChest', 'Endotracheal_tube_PadChest', 'Tracheostomy_tube_PadChest']]\n",
    "        \n",
    "        class_eces_ann = classwise_ECE(y_true_ann_df.to_numpy(), probs_model_df.to_numpy(), classes_list=classes_list, power=power, bins=bins)\n",
    "        all_model_eces_ann.append(class_eces_ann)\n",
    "        \n",
    "        class_eces_pad = classwise_ECE(y_true_pad_df.to_numpy(), probs_model_df.to_numpy(), classes_list=classes_list, power=power, bins=bins)\n",
    "        all_model_eces_pad.append(class_eces_pad)\n",
    "        \n",
    "    #print(all_model_eces_ann)\n",
    "    #print(all_model_eces_pad)\n",
    "    \n",
    "    for c_idx, c in enumerate(classes_list):\n",
    "        print('Class: ', c)\n",
    "        print('Average CWCE Ann: ', round(sum([all_model_eces_ann[i][c_idx] for i in range(3)]) / 3, 5), 'with standard deviation: ', round(statistics.stdev([all_model_eces_ann[i][c_idx] for i in range(3)]), 5))\n",
    "#        print('Average CWCE Ann: ', sum([all_model_eces_ann[i][c_idx] for i in range(3)]) / 3, 'with standard deviation: ', statistics.stdev([all_model_eces_ann[i][c_idx] for i in range(3)]))\n",
    "        print('Average CWCE Pad: ', round(sum([all_model_eces_pad[i][c_idx] for i in range(3)]) / 3, 5), 'with standard deviation: ', round(statistics.stdev([all_model_eces_pad[i][c_idx] for i in range(3)]), 5))\n",
    "        print()\n",
    "    \n",
    "    return preds_df\n",
    "    # Right now, not printing the average class-wise calibration error\n",
    "\n",
    "\n",
    "def classwise_ECE_three_models_CXR14(df_orig_preds, df_y_true, classes_list, power=1, bins=10):\n",
    "        \n",
    "    # Creating the preds df\n",
    "    preds_df = get_perf_only_chd(df_orig_preds, df_y_true, print_auc=False)\n",
    "    all_model_eces = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        probs_model_df = preds_df[['preds_CheD_model'+str(i+1)]]\n",
    "        y_true_ann_df = preds_df[['Chest_drain_Ann']]\n",
    "\n",
    "        class_eces = classwise_ECE(y_true_ann_df.to_numpy(), probs_model_df.to_numpy(), classes_list=classes_list, power=power, bins=bins)\n",
    "        all_model_eces.append(class_eces)\n",
    "        \n",
    "    #print(all_model_eces)\n",
    "    \n",
    "    for c_idx, c in enumerate(classes_list):\n",
    "        print('Class: ', c)\n",
    "        print('Average CWCE Ann: ', round(sum([all_model_eces[i][c_idx] for i in range(3)]) / 3, 5), 'with standard deviation: ', round(statistics.stdev([all_model_eces[i][c_idx] for i in range(3)]), 5))\n",
    "#        print('Average CWCE Ann: ', sum([all_model_eces[i][c_idx] for i in range(3)]) / 3, 'with standard deviation: ', statistics.stdev([all_model_eces[i][c_idx] for i in range(3)]))\n",
    "        print()\n",
    "    \n",
    "    return preds_df\n",
    "    # Right now, not printing the average class-wise calibration error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e455c3",
   "metadata": {},
   "source": [
    "### Multiclass: DenseNet121 fine-tuned on PadChest, detecting 4 tubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7c464204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  Chest_drain_tube\n",
      "Average CWCE Ann:  0.05564 with standard deviation:  0.00089\n",
      "Average CWCE Pad:  0.01045 with standard deviation:  0.00181\n",
      "\n",
      "Class:  NSG_tube\n",
      "Average CWCE Ann:  0.52769 with standard deviation:  0.00778\n",
      "Average CWCE Pad:  0.04563 with standard deviation:  0.00973\n",
      "\n",
      "Class:  Endotracheal_tube\n",
      "Average CWCE Ann:  0.27189 with standard deviation:  0.01031\n",
      "Average CWCE Pad:  0.08022 with standard deviation:  0.00929\n",
      "\n",
      "Class:  Tracheostomy_tube\n",
      "Average CWCE Ann:  0.11031 with standard deviation:  0.00506\n",
      "Average CWCE Pad:  0.10173 with standard deviation:  0.00428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tube_types = ['Chest_drain_tube', 'NSG_tube', 'Endotracheal_tube', 'Tracheostomy_tube']\n",
    "preds_df = classwise_ECE_three_models(TD_preds, true_labels, classes_list=tube_types, power=1, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba89cd0",
   "metadata": {},
   "source": [
    "### Single class: DenseNet121 fine-tuned on PadChest, evaluated on CXR14, detecting only chest drains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f092b93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  Chest_drain_tube\n",
      "Average CWCE Ann:  0.29346 with standard deviation:  0.0242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tube_types = ['Chest_drain_tube']\n",
    "preds_df = classwise_ECE_three_models_CXR14(TD_preds_CXR14, true_labels_CXR14, classes_list=tube_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e3bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
